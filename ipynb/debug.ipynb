{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d177075d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c167e1be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6c0fc86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/notebooks/v2\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6307c146",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['run.py', 'data', 'predictions.npz']\n",
      "run.py--data--predictions.npz\n",
      "Global seed set to 0\n",
      "data\n",
      "predictions.npz\n",
      "['tr.csv', 'TEST.feather', 'XT.pt', '5000.json', 'click_categories.csv', '10000.json', 'sample_submission.csv', 'clickstreams_events.feather', 'bank.feather', 'transactions.feather', 'tr.feather', 'rtk.feather', '10000.feather', '5000.feather', 'currency_rk.csv', 'transactions_events_uids.feather', 'cl.csv', 'mcc_codes.csv', '19623.feather', '15000.feather', 'TRAIN.feather', 'transactions_events.feather', 'train_matching.csv', 'clickstreams_events_uids.feather', '15000.json', 'cl.feather', 'XC.pt', 'YC.pt', 'puzzle.csv', 'YT.pt', '19623.json']\n",
      "14797017\n",
      "100%|███████████████████████████| 14797017/14797017 [00:54<00:00, 269124.38it/s]\n",
      "100%|█████████████████████████████| 2611403/2611403 [00:07<00:00, 328447.20it/s]\n",
      "Sequential2Splitter __init__...\n",
      "Sequential2Splitter run...\n",
      "Sequential2Splitter get_tensors...\n",
      "100%|██████████████████████████████████████| 2463/2463 [00:14<00:00, 164.37it/s]\n",
      "Sequential2Splitter pad...\n",
      "100%|██████████████████████████████████| 16636/16636 [00:00<00:00, 57436.36it/s]\n",
      "100%|██████████████████████████████████| 16636/16636 [00:00<00:00, 22933.63it/s]\n",
      "torch.Size([16636, 2256])\n",
      "torch.Size([16636])\n",
      "rtk_uids 2463\n",
      "Sequential2Splitter get_tensors...\n",
      "100%|██████████████████████████████████████| 2930/2930 [00:22<00:00, 132.86it/s]\n",
      "Sequential2Splitter pad...\n",
      "100%|█████████████████████████████████| 37225/37225 [00:00<00:00, 376517.97it/s]\n",
      "100%|█████████████████████████████████| 37225/37225 [00:00<00:00, 290993.71it/s]\n",
      "torch.Size([37225, 120])\n",
      "torch.Size([37225])\n",
      "bank_uids 2930\n",
      "f8dc2a144b2ebdfd78e0626fdb789a3b\n",
      "Using 16bit native Automatic Mixed Precision (AMP)\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "last\n",
      "Traceback (most recent call last):\n",
      "  File \"/notebooks/v2/main.py\", line 221, in <module>\n",
      "    main()\n",
      "  File \"/notebooks/v2/main.py\", line 185, in main\n",
      "    BB = trainer.predict(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\", line 993, in predict\n",
      "    return self._call_and_handle_interrupt(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\", line 685, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\", line 1037, in _predict_impl\n",
      "    results = self._run(model, ckpt_path=self.predicted_ckpt_path)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\", line 1131, in _run\n",
      "    self._data_connector.prepare_data()\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py\", line 154, in prepare_data\n",
      "    self.trainer.datamodule.prepare_data()\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/pytorch_lightning/core/datamodule.py\", line 474, in wrapped_fn\n",
      "    fn(*args, **kwargs)\n",
      "  File \"/notebooks/v2/lib/data/datamodule/test.py\", line 21, in prepare_data\n",
      "    s.P = s.load(s.name+'.feather').sample(\n",
      "TypeError: unsupported operand type(s) for +: 'ellipsis' and 'str'\n"
     ]
    }
   ],
   "source": [
    "!python -u run.py 'data' 'predictions.npz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f4e3629",
   "metadata": {},
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "1/0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b9bf68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d35b6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142be250",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.base import *\n",
    "from lib.data.splitter.sequential2 import Sequential2Splitter\n",
    "from lib.run.args import Args\n",
    "from lib.data.datamodule.test2 import Test2\n",
    "from lib.run.model import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fffbb7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from catboost import CatBoostClassifier\n",
    "import gc\n",
    "\n",
    "\n",
    "# data, output_path = sys.argv[1:]\n",
    "data = 'data'\n",
    "output_path = 'predictions.npz'\n",
    "\n",
    "\n",
    "transactions = pd.read_csv(\n",
    "    f'{data}/tr.csv')\n",
    "bankclient_embed = transactions.pivot_table(index = 'user_id', \n",
    "                        values=['transaction_amt'],\n",
    "                        columns=['mcc_code'],\n",
    "                        aggfunc=['sum','mean', 'count']).fillna(0)\n",
    "bankclient_embed.columns = [f'{str(i[0])}-{str(i[2])}' for i in bankclient_embed.columns]\n",
    "\n",
    "del transactions\n",
    "gc.collect()\n",
    "\n",
    "clickstream = pd.read_csv(\n",
    "    f'{data}/cl.csv')\n",
    "clickstream_embed = clickstream.pivot_table(index = 'user_id', \n",
    "                        values=['timestamp'],\n",
    "                        columns=['cat_id'],\n",
    "                        aggfunc=['count']).fillna(0)\n",
    "clickstream_embed.columns = [f'{str(i[0])}-{str(i[2])}' for i in clickstream_embed.columns]\n",
    "clickstream_embed.loc[0] = np.empty(len(clickstream_embed.columns))\n",
    "\n",
    "del clickstream\n",
    "gc.collect()\n",
    "\n",
    "dtype_clickstream = list()\n",
    "for x in clickstream_embed.dtypes.tolist():\n",
    "    if x=='int64':\n",
    "        dtype_clickstream.append('int16')\n",
    "    elif(x=='float64'):\n",
    "        dtype_clickstream.append('float32')\n",
    "    else:\n",
    "        dtype_clickstream.append('object')\n",
    "\n",
    "dtype_clickstream = dict(zip(clickstream_embed.columns.tolist(),dtype_clickstream))\n",
    "clickstream_embed = clickstream_embed.astype(dtype_clickstream)\n",
    "\n",
    "dtype_bankclient = list()\n",
    "for x in bankclient_embed.dtypes.tolist():\n",
    "    if x=='int64':\n",
    "        dtype_bankclient.append('int16')\n",
    "    elif(x=='float64'):\n",
    "        dtype_bankclient.append('float32')\n",
    "    else:\n",
    "        dtype_bankclient.append('object')\n",
    "\n",
    "dtype_bankclient = dict(zip(bankclient_embed.columns.tolist(),dtype_bankclient))\n",
    "bankclient_embed = bankclient_embed.astype(dtype_bankclient)\n",
    "\n",
    "list_of_rtk = list(clickstream_embed.index.unique())\n",
    "list_of_bank= list(bankclient_embed.index.unique())\n",
    "\n",
    "submission = pd.DataFrame(list_of_bank, columns=['bank'])\n",
    "submission['rtk'] = submission['bank'].apply(lambda x: list_of_rtk)\n",
    "\n",
    "with open(\"full_list_of_features\", \"rb\") as fp:   # Unpickling\n",
    "    full_list_of_features = pickle.load(fp)\n",
    "\n",
    "model = CatBoostClassifier()\n",
    "model.load_model('model.cbm',  format='cbm')\n",
    "\n",
    "submission_ready = []\n",
    "\n",
    "batch_size = 200\n",
    "# batch_size = 20*2\n",
    "num_of_batches = int((len(list_of_bank))/batch_size)+1\n",
    "\n",
    "for i in tqdm(range(num_of_batches)):\n",
    "    bank_ids = list_of_bank[(i*batch_size):((i+1)*batch_size)]\n",
    "    if len(bank_ids) != 0:\n",
    "        part_of_submit = submission[submission['bank'].isin(bank_ids)].explode('rtk')\n",
    "        part_of_submit = part_of_submit.merge(bankclient_embed, how='left', left_on='bank', right_index=True\n",
    "                                    ).merge(clickstream_embed, how='left', left_on='rtk', right_index=True).fillna(0)\n",
    "\n",
    "        for i in full_list_of_features:\n",
    "            if i not in part_of_submit.columns:\n",
    "                part_of_submit[i] = 0\n",
    "\n",
    "\n",
    "        part_of_submit['predicts'] = model.predict_proba(\n",
    "            part_of_submit[full_list_of_features],\n",
    "#             task_type='GPU'\n",
    "        )[:,1]\n",
    "        part_of_submit = part_of_submit[['bank', 'rtk', 'predicts']]\n",
    "\n",
    "        zeros_part = pd.DataFrame(bank_ids, columns=['bank'])\n",
    "        zeros_part['rtk'] = 0.\n",
    "        zeros_part['predicts'] = 3.8\n",
    "\n",
    "        part_of_submit = pd.concat((part_of_submit, zeros_part))\n",
    "\n",
    "        part_of_submit = part_of_submit.sort_values(by=['bank', 'predicts'], ascending=False).reset_index(drop=True)\n",
    "        part_of_submit = part_of_submit.pivot_table(index='bank', values='rtk', aggfunc=list)\n",
    "        part_of_submit['rtk'] = part_of_submit['rtk'].apply(lambda x: x[:100])\n",
    "        part_of_submit['bank'] = part_of_submit.index\n",
    "        part_of_submit = part_of_submit[['bank', 'rtk']]\n",
    "        submission_ready.extend(part_of_submit.values)\n",
    "\n",
    "submission_final = np.array(submission_ready, dtype=object)\n",
    "\n",
    "print(submission_final.shape)\n",
    "print(submission_final)\n",
    "np.savez(output_path, submission_final)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145f600f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn=np.load('npz/nn.npz',allow_pickle=True)['arr_0']\n",
    "len(nn[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e39632b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cb=np.load('npz/cb.npz',allow_pickle=True)['arr_0']\n",
    "len(cb[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e2e2f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b7b346",
   "metadata": {},
   "outputs": [],
   "source": [
    "1/0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5aa7bf2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b255c797",
   "metadata": {},
   "outputs": [],
   "source": [
    "1/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8056f65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd92960",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 2930\n",
    "tr = io.load('data/transactions.feather')\n",
    "tr = tr[tr.user_id.isin(\n",
    "    tr.user_id.unique()[:k])]\n",
    "assert tr.user_id.nunique()==k\n",
    "io.save(tr,'data/tr.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04a6063",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 2463\n",
    "cl = io.load('data/5000.feather')\n",
    "cl = cl[cl.user_id.isin(\n",
    "    cl.user_id.unique()[:k])]\n",
    "assert cl.user_id.nunique()==k\n",
    "io.save(cl,'data/cl.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b29190",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2313b5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_dir, pred_file =sys.argv[1].split('--')[1:]\n",
    "# data_dir = Path(data_dir)\n",
    "# pred_file = Path(pred_file)\n",
    "# print(data_dir) #/data\n",
    "# print(pred_file) #/output/predictions.npz\n",
    "# print(os.listdir(data_dir)) #['clickstream.csv', 'transactions.csv']\n",
    "\n",
    "data_dir = Path('data')\n",
    "pred_file = Path('predictions.npz')\n",
    "\n",
    "root = Path('json')\n",
    "event_encoder = Encoder(root,'event')\n",
    "event_encoder.load()\n",
    "uid_encoder = Encoder(root,'uid') \n",
    "uid_encoder.load()\n",
    "uid = 'user_id'\n",
    "ts = 'timestamp'\n",
    "\n",
    "#cl = pd.read_csv(data_dir/'clickstream.csv')\n",
    "cl = pd.read_csv(data_dir/'cl.csv')\n",
    "# print(len(cl))\n",
    "# if len(cl)>1000:\n",
    "#     1/0   \n",
    "\n",
    "cl[ts] = cl[ts].progress_apply(pd.Timestamp)\n",
    "event = 'cat_id'\n",
    "cl[event] = cl[event].apply(\n",
    "    lambda x: f'rtk_{x}')\n",
    "cl[event]=event_encoder.transform(cl[event])\n",
    "cl[uid] = uid_encoder.transform(cl[uid])\n",
    "\n",
    "# tr = pd.read_csv(\n",
    "#     data_dir/'transactions.csv')\n",
    "tr = pd.read_csv(data_dir/'tr.csv')\n",
    "tr = tr.rename(\n",
    "    columns={'transaction_dttm': ts})\n",
    "tr[ts] = tr[ts].progress_apply(pd.Timestamp)\n",
    "event = 'mcc_code'\n",
    "tr[event] = tr[event].apply(\n",
    "    lambda x: f'bank_{x}')\n",
    "tr[event]=event_encoder.transform(tr[event])\n",
    "tr[uid] = uid_encoder.transform(tr[uid]) \n",
    "\n",
    "bank = sorted(\n",
    "    tr.user_id.unique().tolist())\n",
    "rtk = sorted(\n",
    "    cl.user_id.unique().tolist())   \n",
    "max_len = max(len(bank),len(rtk))\n",
    "df = pd.DataFrame(index=range(max_len))\n",
    "\n",
    "splitter = Sequential2Splitter()\n",
    "XC,YC,XT,YT = splitter.run(cl,tr)\n",
    "#       df['bank'] = (bank+[-1]*len(bank))[:max_len]\n",
    "#       df['rtk'] = (rtk+[-1]*len(rtk))[:max_len]\n",
    "\n",
    "df['bank'] = (bank+bank)[:max_len]\n",
    "df['rtk'] = (rtk+rtk)[:max_len]\n",
    "\n",
    "df = df.fillna(-1)\n",
    "\n",
    "del cl, tr\n",
    "gc.collect()\n",
    "\n",
    "a = Args(\n",
    "    splitter = 'Sequential',\n",
    "    splitter_pp = dict(\n",
    "        n_days_in_sample = 30,\n",
    "        bank_quantile = 0.9,\n",
    "        rtk_quantile = 0.9,\n",
    "    ),    \n",
    "    n_folds = 3,# 1000 == 'full train'\n",
    "    fold = 0,\n",
    "\n",
    "    fit_limit = 1.,\n",
    "    val_limit = 1.,\n",
    "\n",
    "    batch_size = 32,    \n",
    "    lr = 2e-3,\n",
    "    n_epochs = 10,\n",
    "    check_val_every_n_epoch = 1,\n",
    "\n",
    "    bb_pp = dict(\n",
    "        block_size = 16,\n",
    "        hidden_size = 128,\n",
    "        intermediate_size = 128,\n",
    "        num_attention_heads = 1,\n",
    "        num_hidden_layers = 1,\n",
    "        num_random_blocks = 1,\n",
    "    ),\n",
    "\n",
    "    loss = 'MarginLoss',\n",
    "    loss_pp = dict(),\n",
    "\n",
    "    use_unmatched = bool(0),\n",
    "\n",
    "    miner = None,\n",
    "    miner_pp = dict(),\n",
    "\n",
    "    avg_loss = 'mean',\n",
    "    avg_pred = 'mean',\n",
    ")\n",
    "a.update()\n",
    "a.bank_len = XT.shape[1]\n",
    "a.rtk_len = XC.shape[1] \n",
    "\n",
    "def collate(DD):\n",
    "    AB = [A+B for A in 'XY' for B in 'TC']\n",
    "    kk = AB+['MT','MC']+['bank','rtk','M']\n",
    "    B = {k:[] for k in kk}\n",
    "    for D in DD:\n",
    "        for k in B:\n",
    "            if k in D:\n",
    "                B[k].append(D[k])\n",
    "    for k in B:\n",
    "        if k in AB+['MT','MC']:\n",
    "            B[k] = torch.cat(B[k])\n",
    "        if k in ['bank','rtk','M']:\n",
    "            B[k] = torch.tensor(B[k])\n",
    "    return B\n",
    "\n",
    "c = Args()\n",
    "c.event_encoder = event_encoder\n",
    "c.uid_encoder = uid_encoder\n",
    "d = Args()\n",
    "d.P = df\n",
    "d.XT = XT\n",
    "d.XC = XC\n",
    "d.YT = YT\n",
    "d.YC = YC\n",
    "c.test = Test2(d, collate, a)\n",
    "\n",
    "model = Model(a,c)\n",
    "\n",
    "callbacks = [\n",
    "    pl.callbacks.model_checkpoint.ModelCheckpoint(\n",
    "        save_weights_only = bool(0),\n",
    "        filename = '{R1} {MRR} {P}', \n",
    "        monitor = 'R1', \n",
    "        verbose = False,\n",
    "        save_last = bool(1),\n",
    "        save_top_k = 1, \n",
    "        mode = 'max', \n",
    "    ),\n",
    "]\n",
    "trainer = pl.Trainer(\n",
    "    accumulate_grad_batches = a.acc_batches,\n",
    "#            val_check_interval=a.val_check_interval,\n",
    "    check_val_every_n_epoch=a.check_val_every_n_epoch,\n",
    "    num_sanity_val_steps = 0,\n",
    "    deterministic = bool(0) if a.avg_loss=='median' or a.avg_pred=='median' else bool(1),\n",
    "    benchmark = bool(1),\n",
    "    gpus = a.gpus,\n",
    "    precision = a.precision,\n",
    "    logger = pl.loggers.CSVLogger(\n",
    "        str(a.log_dir), name=a.exp_name),\n",
    "    callbacks = callbacks,\n",
    "    max_epochs = a.n_epochs,\n",
    "    limit_train_batches = a.fit_limit,\n",
    "    limit_val_batches = a.val_limit,\n",
    ")   \n",
    "\n",
    "p = a.log_dir\n",
    "p /= a.exp_name\n",
    "p /= 'version_0'\n",
    "p /= 'checkpoints'\n",
    "a.ckpt = p/'last.ckpt'\n",
    "print(a.ckpt.stem)\n",
    "BB = trainer.predict(\n",
    "    model, c.test, ckpt_path=a.ckpt)  \n",
    "pred = model.predict_epoch_end(BB)\n",
    "#         print(pred.sample().T)\n",
    "\n",
    "path = a.csv_dir/f'{a.ckpt.stem}.csv'\n",
    "c = 'rtk_list'\n",
    "#         pred[c] = pred[c]\\\n",
    "#             .apply(lambda x: str(x))\\\n",
    "#             .replace(\"'\", '', regex=True)\n",
    "pred[c] = pred[c].apply(\n",
    "    lambda x: ([0.0, 0]+x)[:100])\n",
    "print(pred.values)\n",
    "np.savez(str(pred_file), pred.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662d32a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a77f7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28b6f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.rename(\n",
    "    'predictions.npz',\n",
    "    'npz/nn.npz'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f904e361",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.load(\n",
    "    'predictions.npz', allow_pickle=bool(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b0c085",
   "metadata": {},
   "outputs": [],
   "source": [
    "x['arr_0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b54c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2187e910",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183c972e",
   "metadata": {},
   "outputs": [],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d415cae8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77abd5f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822ffe4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbca27ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "cl.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b77e448",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4eeb1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    " len((rtk+rtk+rtk+rtk+rtk+rtk)[:max_len])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449a0a61",
   "metadata": {},
   "outputs": [],
   "source": [
    " len((bank+bank+bank+bank+bank+bank+bank)[:max_len])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7c537d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
